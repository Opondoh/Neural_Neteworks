{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLT Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to practising the process of loading data, regularizing it and using it to train a model, the goal of this assignment is to investigat the effect of changing model parameters on the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be going a bit further with the robot collision dataset. This time, instead of looking at just the first file, we'll look at all five different tasks combined (lp1.data to lp5.data). Prepare two different arrays, X1 and X2, as follows:\n",
    "- Each element in X1 is the immediate reading of the force and torque values after an event, [f1, f2, f3, t1, t2, t3]. The first element should be [1, 1, 63, -3, -1, 0]\n",
    "- Each element in X2 contains 18 values in total - the first, fifth and tenth sets of sensor readings after an event. The first element should be [-1, -1, 63, -3, -1, 0, -1, -1, 63, -3, -1, 0, -1, -1, 61, -3, 0, 0]\n",
    "\n",
    "y should contain the corresponding classes, represented as integers according the the provided dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['robot_execution_failure/lp1.data', 'robot_execution_failure/lp2.data',\n",
    "'robot_execution_failure/lp3.data', 'robot_execution_failure/lp4.data', 'robot_execution_failure/lp5.data']\n",
    "classes = {'normal':0, 'collision':1, 'obstruction':2, 'fr_collision':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal\\n', '\\t-1\\t-1\\t63\\t-3\\t-1\\t0\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load files data\n",
    "f_data = [] #introduce empty array\n",
    "\n",
    "#loop through the different files opening them and adding them into the empty array initiated\n",
    "for i in range (len(files)):\n",
    "    f_single = open(files[i])\n",
    "    data = f_single.readlines()\n",
    "    all_data = f_data.extend(data)\n",
    "f_data[:2] #the first 2 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, 63, -3, -1, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = [] # inputs\n",
    "y = [] # true values\n",
    "classes = {'normal':0, 'collision':1, 'obstruction':2, 'fr_collision':3}\n",
    "\n",
    "# iterate over the lines of the file. If a line matches one of our classes, we split the next line \n",
    "# to get the six readings and use those as our features. \n",
    "for i in range(len(f_data) - 1):\n",
    "    line = f_data[i].strip() # .strip() removes the line endings \\n\n",
    "    if line in classes.keys(): # If the line matches one of our classes (for eg, 'normal')\n",
    "        features = [int(x) for x in f_data[i+1].strip().split('\\t')] # Split the next line to get our features\n",
    "        X1.append(features)\n",
    "        y.append(classes[line]) # And record which class this set of features belongs to\n",
    "X1[0] #show first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, 63, -3, -1, 0, -1, -1, 63, -3, -1, 0, -1, -1, 61, -3, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = [] # inputs\n",
    "y = [] # true values\n",
    "classes = {'normal':0, 'collision':1, 'obstruction':2, 'fr_collision':3}\n",
    "\n",
    "# iterate over the lines of the file. If a line matches one of our classes, we split the next line \n",
    "# to get the six readings and use those as our features. \n",
    "for i in range(len(f_data) - 1):\n",
    "    line = f_data[i].strip() # .strip() removes the line endings \\n\n",
    "    if line in classes.keys(): # If the line matches one of our classes (for eg, 'normal')\n",
    "        #add the first, fifth and tenth sets of sensor readings after an event.\n",
    "        features = [int(x) for x in f_data[i+1].strip().split('\\t')] + [int(x) for x in f_data[i+5].strip().split('\\t')] + [int(x) for x in f_data[i+10].strip().split('\\t')]\n",
    "        X2.append(features)\n",
    "        y.append(classes[line]) # And record which class this set of features belongs to\n",
    "X2[0] #show first element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: establishing a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using techniques covered in this unit, split X1 and y into separate training and testing sets. Use the training set to train a neural network (MLPClassifier) using default parameters but with hidden_layer_sizes=(20, 20, 20). Use the test data you held back to score the model you have created. How well does it perform? Print out the score and confusion matrix. For more accuracy, run through these steps 10 times and find the average score - bonus points for running more times and getting a standard deviation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing to scale the inputs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the inputs \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# Creating the neural network\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivianopondoh/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(20, 20, 20), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25  0  0  0]\n",
      " [ 9 14  0  0]\n",
      " [ 6  5  4  0]\n",
      " [ 5  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "predictions = mlp.predict(X_test)\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6323529411764706\n"
     ]
    }
   ],
   "source": [
    "print(mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1_mean    0.677941\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score avrage\n",
    "import pandas as pd \n",
    "df = pd.DataFrame({\"X1_mean\":[0.7058823529411765, 0.7205882352941176, 0.7352941176470589, 0.5441176470588235, 0.5588235294117647, 0.6764705882352942, 0.6323529411764706, 0.6911764705882353, 0.7647058823529411, 0.75]})\n",
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1_mean    0.075273\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score std\n",
    "df_20 = pd.DataFrame({\"X1_mean\":[0.7058823529411765, 0.7205882352941176, 0.7352941176470589, 0.5441176470588235, 0.5588235294117647, 0.6764705882352942, 0.6323529411764706, 0.6911764705882353, 0.7647058823529411, 0.75, 0.6323529411764706, 0.5735294117647058, 0.45588235294117646, 0.6470588235294118, 0.6470588235294118, 0.6176470588235294, 0.6764705882352942, 0.6323529411764706, 0.6764705882352942, 0.6323529411764706]})\n",
    "df_20.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're getting a convergence warning, you could try having the model train over more iterations - change max_iter = 1000 or 10,000. Does this improve the average score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: adding more inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use X2 in place of X1 - does the score increase or decrease? Was this what you expected? How many samples are there in our training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7058823529411765, 0.7205882352941176, 0.7352941176470589, 0.5441176470588235, 0.5588235294117647, \n",
    "0.6764705882352942, 0.6323529411764706, 0.6911764705882353, 0.7647058823529411, 0.75\n",
    "0.6323529411764706, 0.5735294117647058, 0.45588235294117646, 0.6470588235294118, 0.6470588235294118\n",
    "0.6176470588235294, 0.6764705882352942, 0.6323529411764706, 0.6764705882352942, 0.6323529411764706\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to X1 as our input. Add an extra feature to each item in the array to represent the total force $f_t$. Assume:\n",
    "\n",
    "$f_t^2 = f_1^2 + f_2^2 + f_3^2$\n",
    "\n",
    "Your first input should now look like this:\n",
    "X1[0] = [-1, -1, 61, -3, 0, 0, 61.0163912403872]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the steps from step 2. *Has this extra feature improved model performance?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create at least 3 more models, adding features or changing the number and size of the hidden layers. Print out the average score for your best model. Comment on what you've found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
